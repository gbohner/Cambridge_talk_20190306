{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction to Machine Learning - Dimensionality reduction\n\nThe data we collect often lives in a very high dimensional space. Examples include storing each pixel of an image, the movie preferences of each individual, or even a 500 dimensional feature vector describing a whiskey. \n\nThe aim of dimensionality reduction is two-fold:\n* Understanding the important features (and indeed, combination thereof) of the data\n* Getting rid of variability that could have been caused by random process\n\n<br>\nIn this tutorial we aim to examine a number of widely used algorithms for dimensionality reduction, as well as some more involved techniques. Implementing these algorithms using only linear algebraic operations drives a better understanding of how parameters affect the outcomes of algorithms and how mistakes may be committed, that affect subsequent analyses.\n\n<b><u>Table of contents</u></b>\n\n<a href='#pca'>Principle Component Analysis (PCA)</a>\n\n<a href='#mds'>Multidimensional Scaling (MDS)</a>\n\n<a href='#lle'>Locally Linear Embedding (LLE)</a>\n\n<a href='#isomap'>Isomap</a>\n\n<a href='#further'>Further algorithms</a>\n* ICA, MVU, SNE\n\n<a href='#yours'>Try your own data and algorithms</a>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Toolboxes used\nimport numpy as np # For linear algebra\nimport scipy.io # For loading matlab datasets\nfrom sklearn import decomposition as decomp # PCA-like methods\nfrom sklearn import manifold # Manifold-based methods\nfrom sklearn import neighbors\n\nimport matplotlib # Matlab style plotting package for python\nimport matplotlib.pyplot as plt\n\nnp.set_printoptions(precision=3)\n\nimport dimred_funcs as df # Importing the custom subfunctions from a separate file",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Set matplotlib backend to interactive notebook style\n%matplotlib notebook\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Set figure DPI to lower for smaller images\nmatplotlib.rcParams['figure.dpi'] = 50",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading an example dataset\nX = scipy.io.loadmat('freyface.mat')['X'].astype(float)\nprint('The dimensions of our dataset are: {}').format(X.shape) # The size of the data matrix X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Explore the data\ndf.showfreyface(X[:,0:60]) # Shows the images in the selected columns of the data matrix X (takes a few seconds)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a id='pca'></a>\n## Principle Component Analysis (PCA)\nApply PCA to find the eigendirections of the data"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Built-in PCA results\npca = decomp.PCA(n_components=min(X.shape))\npca.fit(X.T)\neigvec = pca.components_.T\neigval = pca.explained_variance_\nprint_n = 16\nprint('The first {} PCA eigenvalues are {}.\\n').format(print_n, eigval[0:print_n])\n# df.showfreyface(eigvec.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### PCA implementation\n* What's wrong with the code below, why does it give you different values? What do you notice about these values? How would you correct it? (The first line computes the eigendecomposition of $ XX^T $ )"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Wrong implementation\nDun,Vun = np.linalg.eig(np.dot(X,X.T)) # Get eigenvalues and eigenvectors of XX^T\n\norder = Dun.argsort()[::-1] # Get the descending ordering of eigenvalues\nDun = Dun[order]\nVun = Vun[:,order]\n\nprint_n = 16;\nprint('The first {} incorrect PCA eigenvalues are {}.\\n').format(print_n, Dun[0:print_n])\n\n# df.showfreyface(Vun[:,0:16])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Correct the code below to perform PCA on the data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your implementation\n## Write Code Here\nXctr = X\n\nDctr,Vctr = np.linalg.eig(np.dot(Xctr,Xctr.T)) # Get eigenvalues and eigenvectors of the data covariance matrix\n\n\n## End Coding Here\n\norder = Dctr.argsort()[::-1] # Get the descending ordering of eigenvalues\nDctr = Dctr[order]\nVctr = Vctr[:,order]\n\nprint_n = 16;\nprint('Compare your results to the correct eigenvalues.\\n')\nprint('{} {}').format('Correct'.rjust(20), 'Yours'.rjust(20))\nfor i in range(print_n):\n    print('{0:20.3f} {1:20.3f}').format(eigval[i], Dctr[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n### PCA eigenvectors\n* Look at the top 16 eigenvectors in each case. Can you interpret them? How do they differ?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncentered eigenvectors\ndf.showfreyface(Vun[:,0:16], ndims=[4,4], scale=0.6, figtitle='Uncentered')\n\n#PCA eigenvectors\ndf.showfreyface(eigvec[:,0:16], ndims=[4,4], scale=0.6, figtitle='PCA')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n* Project the data onto the top two eigenvectors, and plot the resulting 2D points. Clicking in the scatter plot will reconstruct the corresponding image on the right"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculating data projection\nY = np.dot(eigvec[:,0:2].T, X)\n\n\n# Visualization\nmatplotlib.rcParams['figure.figsize'] = (12,8)\nfig = plt.figure();\nax2 = fig.add_subplot(122)\nax1 = fig.add_subplot(121)\nplt.scatter(Y[0,:], Y[1,:])\nax1.set_xlabel('PC1')\nax1.set_ylabel('PC2')\nYhat = np.array([1,0])\nXhat = np.dot(eigvec[:,0:2], Yhat)\nax2.imshow((Xhat+X.mean(1)).reshape((28,20)), cmap='gray')\nax2.axis('off')\ndef onclick_pca(event):\n    Yhat = np.array([event.xdata, event.ydata])\n    Xhat = np.dot(eigvec[:,0:2], Yhat)\n    ax2.imshow((Xhat+X.mean(1)).reshape((28,20)), cmap='gray')\n    fig.suptitle(Yhat)\ncid = fig.canvas.mpl_connect('button_press_event', onclick_pca)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": " ### PCA eigenspectrum\n * Look at the eigenspectrum. What might be a good choice for k? Is it easy to tell?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Examine the resulting eigenvalues\nplt.figure();\nplt.plot(eigval)\nplt.xlabel('PCA direction')\nplt.ylabel('Eigenvalue')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n* Another metric for choosing k is looking at the cumulative variance explained by the first k eigendirections. Is it easier to select k now?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Examine the resulting eigenvalues\nplt.figure();\ncum_explained = np.cumsum(eigval/np.sum(eigval))\nplt.plot(cum_explained*100)\nplt.xlabel('PCA manifold dimensionality')\nplt.ylabel('Percentage of variance explained')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n* To verify this latest metric, look at a set of random images generated using the first k eigenvectors, such that the linear subspace contains d % of the total variance. For which values do the images look most realistic?\n    * Note that the simulated images are from a multivariate Gaussian with zero mean and variances according to the learned eigenvalues, whereas the true image distribution is usually non-normal (remember the scatter plot of the first 2 eigendirections)\n* Why? "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "d = 70; # Percentage of total variance explained\nk = np.argmax(cum_explained > (d/100.0))+1;\nprint('Using {0:4d} dimensions that explain at least {1:3d}% of the total variance').format(k,d)\n\n#Drawing random samples from the correct Gaussian\nnum_samp = 30\nYsamp = np.random.multivariate_normal(np.zeros(k), np.diag(eigval[0:k]), num_samp).T\nXsamp = np.dot(eigvec[:,0:k], Ysamp)\ndf.showfreyface(Xsamp)\n# df.showfreyface(Xsamp+X.mean(1)[:,np.newaxis])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n### PCA for denoising.\n* Try adding different levels of noise n to an image, projecting into the PCA manifold to find $y$, then computing the reconstruction $\\hat{x}$\n* For what combinations of k and n does the reconstruction look \"good\"?\n* How does this relate to the total variance contained in the PCA manifold chosen?\n* How would you define \"noise floor\"?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n = 0.1; # Noise level percentage (Try 0.01, 0.1, 1, 10, 50, 90) \nk = 100; # Try various values or try to \"guess\" the best one\n\n\n# # Alternatively use the reconstruction level d. Is this optimal? Why?\n# d = 100-n;\n# k = np.argmax(cum_explained > (d/100.0))+1;\n# print('Using {0:4d} dimensions that explain at least {1:3f}% of the total variance').format(k,d)\n\n\nn = n / 100.0 * np.sum(eigval) # Converting noise percentage level into variances\n\n# Creating a noise image\nXtrue = X[:,0] # Choose base image here\nXnoisy = Xtrue - X.mean(1) + np.random.multivariate_normal(np.zeros(Xtrue.shape[0]), np.diag(np.ones(Xtrue.shape[0])*n), 1)\n\nYhat = np.dot(eigvec[:,0:k].T, Xnoisy.T) # Projecting into the k-dimensional PCA manifold\nXhat = np.dot(eigvec[:,0:k], Yhat) # Reconstring the image from the PCA coefficients\n\n# Plotting\nfig = plt.figure();\nax0 = fig.add_subplot(131)\nax0.set_title('Original face')\nax1 = fig.add_subplot(132)\nax1.set_title('Noisy face')\nax2 = fig.add_subplot(133)\nax2.set_title('Denoised face')\nax0.imshow(Xtrue.reshape((28,20)), cmap='gray')\nax1.imshow((Xnoisy.T+X.mean(1)[:,np.newaxis]).reshape((28,20)), cmap='gray')\nax2.imshow((Xhat+X.mean(1)[:,np.newaxis]).reshape((28,20)), cmap='gray')\nax0.axis('off')\nax1.axis('off')\nax2.axis('off')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br> <a id='mds'></a>\n## Multidimensional scaling (MDS)\nMDS looks at a different metric of the data, the pairwise similarities between samples. For numerical data (or at least extracted features) this corresponds to the inner product, or the Gram matrix $X^TX$. Note that in PCA we used the covariance or scatter matrix $XX^T$.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Construct the Gram matrix after removing the mean of the data (use $X_{ctr}$)\n* Use MDS to embed into a 2D space and plot the result (see PCA instructions above for help). Verify that you get the same result as PCA (there might well be sign differences).\n* Look at the results of the first two singular vectors [using *np.linalg.svd()* ] . How do they correspond to the results of PCA and MDS?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Using MDS\n# Write code here\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Using SVD\n# Write code here\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br>\n* Time the three equivalent algorithms: PCA, MDS and SVD. Which is fastest? Explore how this depends on the number of data dimensions and on the number of data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "use_dims = 130 # How many dimensions of the data to use (max 560)\n# (note that if you reduce it the images won't make sense anymore, this is purely for timing purposes)\nuse_samples = 30 # How many of the 1965 provided samples to use",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Xctr = X - X.mean(axis=1)[:,np.newaxis]\n\n#Timing PCA\nprint('Timing PCA: ')\n%timeit np.linalg.eig(np.dot(Xctr[0:use_dims,0:use_samples],Xctr[0:use_dims,0:use_samples].T)/use_samples)\nDpca,Vpca = np.linalg.eig(np.dot(Xctr[0:use_dims,0:use_samples],Xctr[0:use_dims,0:use_samples].T)/use_samples)\n\n#Timing MDS\nprint('Timing MDS: ')\n%timeit np.linalg.eig(np.dot(Xctr[0:use_dims,0:use_samples].T, Xctr[0:use_dims,0:use_samples])/use_samples)\nDmds,Vmds = np.linalg.eig(np.dot(Xctr[0:use_dims,0:use_samples].T, Xctr[0:use_dims,0:use_samples])/use_samples)\n\n#Timing SVD\nprint('Timing SVD: ')\n%timeit np.linalg.svd(Xctr[0:use_dims,0:use_samples]/np.sqrt(use_samples))\nVsvd,Dsvd,tmp = np.linalg.svd(Xctr[0:use_dims,0:use_samples]/np.sqrt(use_samples))\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check that the eigenvalues are the same (Note that either PCA or the MDS matrix may be badly conditioned, but not both!)\nprint(Dpca[0:16])\nprint(Dmds[0:16])\nprint(Dsvd[0:16]**2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* Can you think of different metrics to try with MDS?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n## Kernel PCA\n* _[Advanced]_ Try replacing the inner products with squared-exponential (i.e. Gaussian) kernel evaluations. What do you think of these results?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Kernel PCA\n# Write Code here\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br> <a id='lle'></a>\n## Locally linear embedding (LLE)\nLLE is a globally non-linear method that aims to preserve linearity within a certain neighbourhood of each data point\n\nThe 3 steps of the algorithm are:\n1. Compute pairwise distances and find neighborhoods\n2. Solve for reconstruction weights $W$ at each point\n    * Regression from the neighbourhood\n3. Compute embedding from the eigenvectors of the cost matrix $(I-W)^T(I-W)$\n\nThis same face data was used in the original LLE paper (Roweis & Saul, see lecture notes). \n* Can you reproduce the figure? (They used a 12-nearest-neighbour graph and 2 dimensions).\n* Explore the resulting projection space. Does it make more or less sense than the PCA one?\n* Experiment with the neighbourhood size. How does it affect the results? Which neighbourhood gives the most interpretable embedding?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Built-in\nlle = manifold.LocallyLinearEmbedding(n_neighbors=12, n_components=2, reg=0, eigen_solver='dense', \\\n                                      max_iter=100000, method='standard', neighbors_algorithm='brute', \\\n                                      tol=1e-06)\nY = lle.fit_transform(X.T) # Computes the embedded face points\n\n# # Custom written\n# Y = df.custom_lle(X,n_neighbours=12, k=2, regularization=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Visualization\n# Note that reconstruction from an arbitrary point using LLE is somewhat difficult, \n# for now we'll just find the nearest existing data point to the user's click\n\n# Train a neighborhood object within the embedding space\nYnbrs = neighbors.NearestNeighbors(n_neighbors=1)\nYnbrs.fit(Y)\n\nfig = plt.figure();\nax2 = fig.add_subplot(122)\nax1 = fig.add_subplot(121)\nplt.scatter(Y[:,0], Y[:,1])\n#ax1.set_title(('Local {}d embedding').format(lle.n_components))\nax2.axis('off')\ndef onclick_lle(event):\n    Yhat = np.array([event.xdata, event.ydata])\n    # Find nearest neighbour in embedding space\n    Xhatind = Ynbrs.kneighbors(Yhat.T, 1, return_distance=False) # returns the index of the nearest neighbor\n    Xhat = X[:,Xhatind[0]]\n    ax2.imshow(Xhat.reshape((28,20)), cmap='gray')\n    fig.suptitle(Yhat)\ncid = fig.canvas.mpl_connect('button_press_event', onclick_lle)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n* The code above always shows an existing sample, that is closest to the clicked point\n* How would you modify the algorithm to reconstruct an arbitrary sample? Try below!\n<br>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Built-in\nlle = manifold.LocallyLinearEmbedding(n_neighbors=12, n_components=2, reg=1e-03, eigen_solver='dense', \\\n                                      max_iter=100000, method='standard', neighbors_algorithm='brute', \\\n                                      tol=1e-06)\nY = lle.fit_transform(X.T) # Computes the embedded face points\n\n# Visualization\n\n## Modify this code ----------------------------------------------------------------------------------\n\n# Train a neighborhood object within the embedding space\nYnbrs = neighbors.NearestNeighbors(n_neighbors=1)\nYnbrs.fit(Y)\n\n## End modifying code --------------------------------------------------------------------------------\n\nfig = plt.figure();\nax2 = fig.add_subplot(122)\nax1 = fig.add_subplot(121)\nplt.scatter(Y[:,0], Y[:,1])\nax1.set_title(('Local {}d embedding').format(lle.n_components))\nax2.axis('off')\ndef onclick_lle2(event):\n    Yhat = np.array([event.xdata, event.ydata])\n    ## Write code here ----------------------------------------------------------------------------------\n    \n    \n    Xhat = np.zeros(Xhat.shape) # Change this line\n\n\n    ## End code here (that sets the value of Xhat) ------------------------------------------------------\n    ax2.imshow(Xhat.reshape((28,20)), cmap='gray')\n    fig.suptitle(Yhat)\ncid = fig.canvas.mpl_connect('button_press_event', onclick_lle2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* *[Advanced]* How would you implement your own LLE algorithm?\n    * Follow the steps outlined and use the lecture notes for the math"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your LLE\ndef my_lle(X, n_neighbours=12, k=2):\n    #X: raw input data\n    \n    #Step 1 - For each point get the local neighbourhood based on pairwise distance measures\n    # Input - data (features x samples) and n_neighbors. \n    # Output - local neighbourhood indices (n_neighbours x samples)\n    \n    \n    \n    #Step 2 - For each point solve reconstruction using it's neighbourhood as regressors\n    # Input - data,  neighbourhoods. \n    # Output - local reconstruction weights W (n_neighbours x samples)\n    \n    \n    #Step 3 - Compute the best k-dimensional embedding Y by solving the quadratic cost function\n    # Input - W\n    # Output - Y (k x samples)\n    # Note that you do not get \"projection vectors\" as in PCA, only the projected data\n    # This is due to the fact that globally this method is non-linear.\n    \n    # This step is somewhat tricky\n    \n    return",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br> <a id='isomap'></a>\n## Isomap\nIsomap is a globally non-linear method that aims to preserve distances between samples in a graph-path sense\n\nThe 3 steps of the algorithm are:\n1. Compute pairwise distances (euclidean) and find neighborhoods\n2. Recompute pairwise distances using shortest path in graph\n3. Compute embedding using metric MDS\n\nThe Isomap paper (Tenenbaum et al.) used a different face data set. Try running it on Brendan and compare to LLE. \n* Again, how does the neighbourhood affect the results?\n* Can you reconstruct in this case?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Built-in\nisomap = manifold.Isomap(n_neighbors=12, n_components=2, eigen_solver='dense', tol=0, \\\n                         max_iter=None, path_method='FW', neighbors_algorithm='brute')\nY = isomap.fit_transform(X.T) # Computes the embedded face points\n\n# Visualization\n# Note that reconstruction from an arbitrary point using LLE is somewhat difficult, \n# for now we'll just find the nearest existing data point to the user's click\n\n# Train a neighborhood object within the embedding space\nYnbrs = neighbors.NearestNeighbors(n_neighbors=1)\nYnbrs.fit(Y)\n\nfig = plt.figure();\nax2 = fig.add_subplot(122)\nax1 = fig.add_subplot(121)\nplt.scatter(Y[:,0], Y[:,1])\nax1.set_title(('Local {}d embedding').format(lle.n_components))\nax2.axis('off')\ndef onclick_isomap(event):\n    Yhat = np.array([event.xdata, event.ydata])\n    # Find nearest neighbour in embedding space\n    Xhatind = Ynbrs.kneighbors(Yhat.T, 1, return_distance=False) # returns the index of the nearest neighbor\n    Xhat = X[:,Xhatind[0]]\n    ax2.imshow(Xhat.reshape((28,20)), cmap='gray')\n    fig.suptitle(Yhat)\ncid = fig.canvas.mpl_connect('button_press_event', onclick_isomap)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n* *[Advanced]* Try your own implementation of isomap (notice that you can reuse code from lle)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Your Isomap\ndef my_isomap(X, n_neighbours=12, k=2):\n    # Implement the 3 steps of isomap\n    \n    return",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n<a id=\"further\"></a>\n# Further algorithms\n\nIt is very much recommended that whenever you plan to use dimensionality reduction during your research, you dig into it and implement your own version. As you could see above, most of these algorithms are actually quite simple and can be implemented in a few lines of code, if you understand the linear algebra going on in the background."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<br>\n## Independent Component Analysis (ICA)\nICA is not a classical dimensionality reduction method, rather a linear combination representation of the data. It often actually increases the dimensionality of our data, but such that each of the axis is meaningful in some sense \n\n(We'd like to refer to meaningful in the human sense, but we need to somehow translate human meaning into math. We can do this in multiple ways. One way is to assume basis vectors are sparse - meaning they only put weight in a few places, resulting in more interpretable features. Adding further constraint [such as explicitly local features instead of just sparse] result in even more interpretability, but way more complicated algorithms to write).\n\nLook at our favourite Frey dataset in this sense now, and locate the eyebrow feature :-)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ica = decomp.FastICA(n_components=100, algorithm='parallel', whiten=True, fun='logcosh', fun_args=None, \\\n                     max_iter=200, tol=0.0001, w_init=None, random_state=None)\nica.fit(X.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "icavec = ica.components_.T\ndf.showfreyface(icavec)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br>\n## Maximum Variance Unfolding\nUsing the same neighbourhood graph as LLE and Isomap, but locally tries to preserve variance like PCA instead of reconstructability (LLE) or graph distance (Isomap).\n\n* *[Very Advanced]* Add an option to your custom LLE algorithm such that it preserves all local distances in the neighborhoods instead of the reconstructability"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# No built-in version yet\n# Implement my_lle(..., mvu=true)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br>\n## Stochastic Neighbour Embedding (SNE)\nThe previous neighbourhood-preserving algorithms (LLE, Isomap, MVU) always used the same \"hard\"-neighbourhoods (either two samples are neighbours or not). Here we \"soften\" this algorithm and instead use a probability that two points are neighbours."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tsne = manifold.TSNE(n_components=2, perplexity=30.0, early_exaggeration=4.0, learning_rate=1000.0, \\\n                     n_iter=1000, n_iter_without_progress=30, min_grad_norm=1e-07, metric='euclidean', \\\n                     init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)\nY = tsne.fit_transform(X.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Visualization\n# Note that reconstruction from an arbitrary point using LLE is somewhat difficult, \n# for now we'll just find the nearest existing data point to the user's click\n\n# Train a neighborhood object within the embedding space\nYnbrs = neighbors.NearestNeighbors(n_neighbors=1)\nYnbrs.fit(Y)\n\nfig = plt.figure();\nax2 = fig.add_subplot(122)\nax1 = fig.add_subplot(121)\nplt.scatter(Y[:,0], Y[:,1])\n#ax1.set_title(('Local {}d embedding').format(lle.n_components))\nax2.axis('off')\ndef onclick_sne(event):\n    Yhat = np.array([event.xdata, event.ydata])\n    # Find nearest neighbour in embedding space\n    Xhatind = Ynbrs.kneighbors(Yhat.T, 1, return_distance=False) # returns the index of the nearest neighbor\n    Xhat = X[:,Xhatind[0]]\n    ax2.imshow(Xhat.reshape((28,20)), cmap='gray')\n    fig.suptitle(Yhat)\ncid = fig.canvas.mpl_connect('button_press_event', onclick_sne)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<br><br><br>\n<a id='yours'></a>\n# Try your own data and algorithms\n\nThere are lots of datasets available online:\n* ETH unsupervised challange - http://www.causality.inf.ethz.ch/unsupervised-learning.php?page=datasets\n* SKlearn dataset tools - \"from sklearn import datasets\", http://scikit-learn.org/stable/datasets/\n* Kaggle: https://www.kaggle.com/datasets\n\nToolboxes (Python):\n* Scikit-learn (ML toolbox that we used here a lot): http://scikit-learn.org/\n* Pandas (Data analysis toolbox (less machine learning)): http://pandas.pydata.org/\n* Numpy and Scipy (linear algebra, signal processing etc, Write your own custom algorithms in these): https://www.scipy.org/\n\nToolbox (C++, with Python interface):\n* SHOGUN Machine Learning (fast and flexible toolbox): http://www.shogun-toolbox.org/\n* Eigen (amazing linear algebra toolbox): http://eigen.tuxfamily.org/\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}