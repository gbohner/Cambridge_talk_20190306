---
title: "scRNA-seq analysis via Dimensionality reduction"
output:
  html_notebook:
    df_print: paged
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r echo=FALSE}
# Set libraries path
.libPaths(new="/usr/local/lib/R/site-library/3.5")

# Set base knitr options
library(knitr)
opts_chunk$set(
  message=FALSE,
  warning=FALSE,
  max.print = 20,
  rownames.print =TRUE
  )
```


### Technical comments

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. You can also view the current state of the notebook as a well-formatted HTML document, if you click preview just above this document.

Try executing a chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. You can also run the next chunk of code via *Cmd+Alt+`*. (These commands are valid on Mac, might differ on Windows)

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed. If you wish to get a full-fledged HTML output (that includes the interactive plots inside the document, rather than in seperate viewer windows), choose *Knit to HTML* instead of the *Preview Notebook* button.


# Loading libraries

In R (and many other programming language) reusable code written by others is distributed via libraries. In order to use those pieces of code, we first need to install the packages (this has been done previously for all users, as it takes 2+ hours to install all required packages), then we need to load them within our current session.

```{r load libraries, message=FALSE, warning=FALSE}

# Load libraries
library(magrittr) # Pipe %>% operation for clean coding
library(SingleCellExperiment) # Data container (https://bioconductor.org/packages/release/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html)
library(pcaMethods) # Linear DimRed methods (PCA and extensions)
library(RDRToolbox) # Non-linear DimRed methods (LLE, Isomap)
library(ggbiplot)
library(plotly)

library(sparsepca)

library(scran) # scRNA-seq methods, By Aaron Lun, Cambridge researcher

library(tidyverse)

library(printr)


cat("Libraries successfully loaded\n")

```

# Reading and understanding the data



```{r}
# Read the data
data_zeisel <- readRDS("zeisel.rds")

# Get basic information of the data
print(data_zeisel)
```


# Reducing dimensionality of the data

### Step 1 - Remove uninformative genes

The easiest way to reduce the amount of our data is to select only a subset of the recorded genes to analyse.

Before we remove anything, we want to know in the end, how much information we are removing from the data by the subsetting of genes. To do this, we should first know, how much information is in the data. An easy proxy of "information" that we'll use today is the amount of Variance in the gene expression counts across cells data.

```{r}

# In order to compute sample-wise variance, we need to center the data 
# (Column-wise, so basically find the mean of all data and substract that from each data point to serve as the new origin)

center_rowmeans <- function(X) {
    Xmean = rowMeans(X)
    X - ( Xmean %*% t(rep(1,dim(X)[2])))
}

# Get total variance as the trace of the covariance matrix (but to save computation, we can rotate elements in trace as Tr[X X^T] = Tr[X^T X], then realise that this is just = sum(colSums(Xc^2)) = sum(Xc^2))
get_total_matrix_variance <- function(X){
  Xc <- center_rowmeans(X) 
  sum(Xc^2)
}

# Write a convenience function that directly computes the variance in our SCE data
get_total_data_variance <- function(sce_data, logcount = TRUE){
  if (logcount){
    sce_data %>% 
      logcounts() %>%
      get_total_matrix_variance() ->
      out
  } else {
    sce_data %>% 
      counts() %>%
      get_total_matrix_variance() ->
      out
  }
  
  # Return out
  out
}


# Get total variance in the (log counts) data
total_logcounts_variance <- get_total_data_variance(data_zeisel)

sprintf(paste0(
  "The total variance in the logcounts data is ", 
  get_total_data_variance(data_zeisel)
  ))

```

We may then remove genes that have very low total counts in the data:

```{r}
# We first just remove the genes that have to small total counts across the whole dataset.
min_total_count_per_gene = 25
data_zeisel %>% 
  counts() %>% 
  rowSums() -> 
  tmp_gene_total_counts
  
data_zeisel[
  tmp_gene_total_counts >= min_total_count_per_gene, 
] ->
  data_zeisel_lowcount
  

# Of course let's check how much information we still have:
sprintf(paste0(
  "We now have ", nrow(data_zeisel_lowcount), " out of ", nrow(data_zeisel), " genes remaining. ", 
  "After removing low count genes, we retain ", format(
    get_total_data_variance(data_zeisel_lowcount) / total_logcounts_variance *100, 
    digits=3
  ),
  "%% of the variance." 
  ))


```

Next we check from the remaining genes, which one on average have the most variation in fold-changes across the different cells. As our scientific goal currently is to find a way to characterise how cells differ from one-another, this is a reasonable thing to do.


```{r}

how_many_genes_to_keep = 500

# Next we check from the remaining genes, which one on average have the most fold_changes
data_zeisel_lowcount %>% 
  logcounts() %>%  # logcounts assay
  rowVars -> 
  gene_count_variances # save row-wise variances

# Add back the names (unfortunately rowVars deletets them)
names(gene_count_variances) <- rownames(data_zeisel_lowcount)
 
# Do the subsetting based on top N variances (by sorted gene name)
data_zeisel_lowcount[
  names((gene_count_variances %>% sort(decreasing = TRUE))[1:how_many_genes_to_keep]),
  ] ->
  data_zeisel_topgenes

data_zeisel_subset <- data_zeisel_topgenes # Just for compatibility with historical code.

# Of course let's check how much information we still have:
sprintf(paste0(
  "After keeping only the top ", how_many_genes_to_keep, " genes, we retain ", 
  format(
    get_total_data_variance(data_zeisel_topgenes) / total_logcounts_variance *100, 
    digits=3
  ),
  "%% of the variance." 
  ))


```


```{r pca_plotting_function, echo=FALSE}

# Write a plotting function for our PCA results
plot_pca_result <- function(pca_result, dataset=NULL, show_celltype = FALSE, interactive=TRUE){
  if (is.null(pca_result$x)) pca_result$x <- pca_result$scores
  # Plot PCA
  if (!show_celltype){
      P2 <- ggbiplot(pca_result,
                     obs.scale = 1, 
                     var.scale=1,
                     ellipse=F,
                     circle=F,
                     varname.size=0.1,
                     var.axes=F,
                     groups=rep(1, times=dim(results_prcomp$x)[1]), 
                     alpha=0) + 
        theme(legend.position="none")
      P2$layers <- c(geom_point(#aes(color=data_zeisel_subset$cell_type1),
                                cex=1.0, alpha=0.3), P2$layers)
  } else {
    if (is.null(dataset)) stop("Cannot show cell types, no the \"dataset\" input is missing. Try again with show_celltype=FALSE, or supply the dataset the PCA analysis was ran on")
    
    P2 <- ggbiplot(pca_result,
                       obs.scale = 1, 
                       var.scale=1,
                       ellipse=F,
                       circle=F,
                       varname.size=0.1,
                       var.axes=F,
                       groups=dataset$cell_type1, 
                       alpha=0)
    P2$layers <- c(geom_point(aes(color=dataset$cell_type1), 
                              cex=1.0, alpha=0.3), P2$layers)
  }
  
  # Return the ggplot_object
  if (interactive){
    P2_out <- P2
    P2_out$layers[[2]]$mapping$text = aes(text=rownames(pca_result$x))$text # Add cell id as hover text in plotly
    # Show in new browser tab
    htmltools::html_print(
       P2_out %>% ggplotly(), 
       viewer = getOption("browser", utils::browseURL)
    ) 
    # use default viewer for compile
    # htmltools::html_print(
    #    P2_out %>% ggplotly()
    # )
  }
  
  P2
}

```

### Step 2 - Dimensionality reduction via combination of genes

Now we may wish to do some more advanced dimensionality reduction, that takes into account not just properties of single genes, but also how they interact. Principal Component Analysis (PCA) is one such methods, it attempts to reduce the dimensionality of the data not just by getting rid of single genes, but by finding linear combinations of gene expression patterns that are informative, and keep most of the variance intact.

```{r}
# Run PCA on the data
results_prcomp <- prcomp(
  data_zeisel_topgenes %>% logcounts() %>% t() # Tranpose is necessary because base R algorithms think in data frames, where samples are in rows, and features are in columns 
  
  # Also for some weird reason pr_comp sdev values are scaled, such that:
  # EV(X*X^T) = results_prcomp$sdev^2*(n_samples-1)
)


plot_pca_result(results_prcomp, interactive = TRUE)

```


Now what have we learned, and was this helpful? Not really, interactive mode can point out outliers maybe (although PCA is not very robust to them). And what does PC1 and PC2 really mean?

We can look at what the loadings for the PC1 axis are:

```{r}
# Print PC_i coeffs in 
i = 1
tmp_cur_pc_sorted <- results_prcomp$rotation[,i][order(abs(results_prcomp$rotation[,i]), decreasing = TRUE)]
output_string = ""
for (j in 1:min(length(tmp_cur_pc_sorted), 80)){
  output_string = paste0(
    output_string,
    stringr::str_sub(format(tmp_cur_pc_sorted[j],digits=4), 1, 6),
    " * ",
    names(tmp_cur_pc_sorted)[[j]],
    " + "
    )
  if (!mod(j, 4)) output_string = paste0(output_string, "\n")
}

output_string = stringr::str_sub(output_string, end = -4)

cat(paste0("PC1_coordinate = \n", output_string, "\n + ......"))
```

Still not terribly helpful, but this represents some linear combination of (log) gene expressions. It actually means an equation for a single line in the space of all possible expression patterns. For a hypotethical "cell_x", that has expression x_<gene_name> for every gene, this equation would look something like this:


$$ \textrm{PC1} ( \textrm{cell_x} ) = 0.15 * x_\textrm{Plp1} +  0.11 * x_\textrm{Trf} - 0.08 * x_\textrm{Snap25} + \dots + 0.0008 * x_\textrm{Slc6a1} $$

Ok so this means that when some of those genes in the equation are upregulated together, and some others are down-regulated at the same time. That's great, I can go test that?! No, not really, this equation involves ALL measured genes, so it is not particularly useful to create new hypothesis.

What can we do about that?

    ``` 
      First, let's look at what PCA actually does! 
      (Continued on the board) 
    
    ```

Now with our extra penalty term for using too many genes to explain variance, we can make a trade-off between how much variance our PC1 explains, versus how many genes the $w1$ vector involves to compute that PC1.

```{r message=FALSE}

# Run SPCA with various alpha regularisers to increase sparsity (only 100 genes to make it fast)
results_spca = tibble(spca_alpha = c(1e-4, 1e-3, 5e-3, 1e-2, 3e-2, 5e-2))

# Do the fits for each alpha value
results_spca %>%
  mutate(
    spca_fit = purrr::map(
      spca_alpha, 
      function(a) {
        tmp_data <- data_zeisel_topgenes[1:100,] %>% logcounts() %>% t()
        tmp_out <- sparsepca::spca(
          tmp_data,
          alpha = a,
          verbose=FALSE
        )
        rownames(tmp_out$loadings) <- colnames(tmp_data)
        rownames(tmp_out$scores) <- rownames(tmp_data)
        # Return
        tmp_out
      }
      )
  ) ->
  results_spca

# Compute statistics for each PC1 (with different alpha coefficients)
results_spca %>%
  mutate(
    genes_used_spca =  purrr::map(
      spca_fit,
      function(x){
        sum(abs(x$loadings[,1])>1e-6)
      }),
    var_explained_pc1 = purrr::map(
      spca_fit,
      function(x){
        sum(x$sdev[[1]]^2)
      })
  ) %>%
  unnest(genes_used_spca, var_explained_pc1) ->
  results_spca

# Run basic PCA on the same (restricted) data
results_prcomp_100genes <- prcomp(
  data_zeisel_topgenes[1:100,] %>% logcounts() %>% t()
)

results_spca %>%
  mutate(
    `PCA_info_kept_%` = var_explained_pc1 / results_prcomp_100genes$sdev[[1]]^2 *100,
    genes_used_pca = sum(abs(results_prcomp_100genes$rotation[,1]) > 1e-6),
    `orig_data_info_kept` = var_explained_pc1 * (ncol(data_zeisel)-1) / total_logcounts_variance * 100
  ) ->
  results_spca

# Show the results as a tibble
results_spca %>%
  select(spca_alpha, genes_used_spca, `PCA_info_kept_%`,  genes_used_pca,  `orig_data_info_kept` )

```

Exampine the SPCA results in terms of scientific interpretability (ie whether the 2 genes that explain a large chunk of variance for $\alpha=0.05$ are actually related?):

```{r}
results_spca %>% 
  filter(spca_alpha==0.05) %>%
  pull(spca_fit) %>%
  unlist(recursive=FALSE) ->
  spca_fit_very_sparse

spca_fit_very_sparse$loadings[,1][abs(spca_fit_very_sparse$loadings[,1])>1e-6]
```

What more we can do with these kinds of results?

```{r}
# Plotting function for spca (doesn't have one on its own, need to make a dummy "princomp" object from it)
get_plottable_spca_obj <- function(spca_fit, add_noise_pc2 = 0){
  out <- spca_fit
  class(out) <- "princomp"
  # Fix the variance explained on the axes for plotting (hacky way! Just for getting the correct "variance explained" on plot axes)
  out$sdev[length(out$sdev)] <- sqrt(spca_fit$var - sum(out$sdev^2))
  out$n.obs = dim(out$scores)[[1]]
  if (!(out$scale)) out$scale = (out$center*0+1)
  class(out$loadings) <- "loadings"
  
  # Add noise along PC2 for visualisation purposes
  out$scores[,2] = out$scores[,2] + rnorm(dim(out$scores)[[1]], 0, add_noise_pc2)
  # Return out
  out
}

plot_pca_result(
  get_plottable_spca_obj(
    spca_fit_very_sparse,
    add_noise_pc2 = 0.5
  ), 
  interactive = TRUE,
  dataset = data_zeisel_topgenes[1:100,], 
  show_celltype = FALSE
)

```

The detected sparse direction only 2 genes (Plp1 and Trf), yet cells are quite spread along this axis! 

I can now quite strongly feel that the expression levels of Plp1 and Trf are significantly different in a group of cells compared to others. This is a testable hypothesis, that I could go back and examine how those cells differ from others (in shape, or electrical activity for example).

Examining the results of the author's clustering algorithm along with our results, we can see that our high-expression level group of cells indeed formed a cluster in their analysis, and were named "Oligodendrocytes", based on some expert knowledge they had. And indeed if we review the literature, we'll see that there has been studies showing these genes are indicative of oligodendrocytes, and are called "myelination genes".

```{r}
# Now also show cell types
plot_pca_result(
  get_plottable_spca_obj(
    spca_fit_very_sparse,
    add_noise_pc2 = 0.5
  ), 
  interactive = TRUE,
  dataset = data_zeisel_topgenes[1:100,], 
  show_celltype = TRUE
)

```



